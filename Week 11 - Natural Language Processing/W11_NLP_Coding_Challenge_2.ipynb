{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marina - NLP Coding Challenge #2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qd1ltfV8QShV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Challenge #2: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "iWWDecybQ1zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A common task in NLP is to determine the similarity between documents or words. In order to facilitate the comparison between documents or words, you will leverage the learnings from Coding Challenge #1 to create vectors. Once you have a document term matrix, comparisons are possible since you can measure the difference between the numbers.\n",
        "\n",
        "In this Coding Challenge, you will utilize the \"**Gensim**\" library, which is a free Python library to determine document similarity.\n",
        "\n",
        "**\"Gensim\" Reference**: https://radimrehurek.com/project/gensim/\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "c3hSkp0srSXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install Gensim**:"
      ]
    },
    {
      "metadata": {
        "id": "tEPlGPVSrZv6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/install.html\n",
        "!pip install --upgrade gensim\n",
        "!pip install regex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56qjZ1IzsO54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install NLTK:**"
      ]
    },
    {
      "metadata": {
        "id": "kfVsL9M9sSMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import the NLTK package\n",
        "import nltk\n",
        "\n",
        "# Get all the data associated with NLTK â€“ could take a while to download all the data\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CC68LogGscTI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import the requiste NLTK packages:**"
      ]
    },
    {
      "metadata": {
        "id": "lt_gawHGsbrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import word tokenizer\n",
        "import numpy as np\n",
        "import regex as re\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hj_FUsxPtDOI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dataset:**"
      ]
    },
    {
      "metadata": {
        "id": "kM4qHcNWs1__",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For the purposes of this challenge, each line represents a document. In all, there are 8 documents\n",
        "\n",
        "raw_documents = ['The dog ran up the steps and entered the owner\\'s room to check if the owner was in the room.',\n",
        "                 'My name is Thomson Comer, commander of the Machine Learning program at Lambda school.',\n",
        "                 'I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.',\n",
        "                 'Machine Learning is one of my favorite subjects.',\n",
        "                 'I am excited about taking the Machine Learning class at the Lambda school starting in April.',\n",
        "                 'When does the Machine Learning program kick-off at Lambda school?',\n",
        "                 'The batter hit the ball out off AT&T park into the pacific ocean.',\n",
        "                 'The pitcher threw the ball into the dug-out.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jKGw63jmrjLg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #1**: **Create a document that contains a list of tokens**"
      ]
    },
    {
      "metadata": {
        "id": "fU5v8oM3t2nx",
        "colab_type": "code",
        "outputId": "4b07c0f9-f691-46fd-a048-1cb32e412004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "cell_type": "code",
      "source": [
        "clean = [re.sub(r'[^\\w\\s]','',i).lower() for i in raw_documents]\n",
        "print('no punctuation: \\n', np.matrix(clean), '\\n')\n",
        "en_stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
        "\n",
        "tokens = [word_tokenize(x) for x in clean]\n",
        "print('tokens: \\n', np.matrix(tokens), '\\n')\n",
        "\n",
        "nontokens = []\n",
        "\n",
        "for i in tokens:\n",
        "  nontokens.append([])\n",
        "  for j in i:\n",
        "    if j in en_stopwords:\n",
        "      continue\n",
        "    else: nontokens[-1].append(j)\n",
        "      \n",
        "print('tokens minus stopwords: \\n', np.matrix(nontokens), '\\n')\n",
        "\n",
        "docs2 = [' '.join(i) for i in nontokens]\n",
        "print('docs minus stopwords: \\n', np.matrix(docs2), '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no punctuation: \n",
            " [['the dog ran up the steps and entered the owners room to check if the owner was in the room'\n",
            "  'my name is thomson comer commander of the machine learning program at lambda school'\n",
            "  'i am creating the curriculum for the machine learning program and will be teaching the fulltime machine learning program'\n",
            "  'machine learning is one of my favorite subjects'\n",
            "  'i am excited about taking the machine learning class at the lambda school starting in april'\n",
            "  'when does the machine learning program kickoff at lambda school'\n",
            "  'the batter hit the ball out off att park into the pacific ocean'\n",
            "  'the pitcher threw the ball into the dugout']] \n",
            "\n",
            "tokens: \n",
            " [[list(['the', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'the', 'owners', 'room', 'to', 'check', 'if', 'the', 'owner', 'was', 'in', 'the', 'room'])\n",
            "  list(['my', 'name', 'is', 'thomson', 'comer', 'commander', 'of', 'the', 'machine', 'learning', 'program', 'at', 'lambda', 'school'])\n",
            "  list(['i', 'am', 'creating', 'the', 'curriculum', 'for', 'the', 'machine', 'learning', 'program', 'and', 'will', 'be', 'teaching', 'the', 'fulltime', 'machine', 'learning', 'program'])\n",
            "  list(['machine', 'learning', 'is', 'one', 'of', 'my', 'favorite', 'subjects'])\n",
            "  list(['i', 'am', 'excited', 'about', 'taking', 'the', 'machine', 'learning', 'class', 'at', 'the', 'lambda', 'school', 'starting', 'in', 'april'])\n",
            "  list(['when', 'does', 'the', 'machine', 'learning', 'program', 'kickoff', 'at', 'lambda', 'school'])\n",
            "  list(['the', 'batter', 'hit', 'the', 'ball', 'out', 'off', 'att', 'park', 'into', 'the', 'pacific', 'ocean'])\n",
            "  list(['the', 'pitcher', 'threw', 'the', 'ball', 'into', 'the', 'dugout'])]] \n",
            "\n",
            "tokens minus stopwords: \n",
            " [[list(['dog', 'ran', 'steps', 'entered', 'owners', 'room', 'check', 'owner', 'room'])\n",
            "  list(['name', 'thomson', 'comer', 'commander', 'machine', 'learning', 'program', 'lambda', 'school'])\n",
            "  list(['creating', 'curriculum', 'machine', 'learning', 'program', 'teaching', 'fulltime', 'machine', 'learning', 'program'])\n",
            "  list(['machine', 'learning', 'one', 'favorite', 'subjects'])\n",
            "  list(['excited', 'taking', 'machine', 'learning', 'class', 'lambda', 'school', 'starting', 'april'])\n",
            "  list(['machine', 'learning', 'program', 'kickoff', 'lambda', 'school'])\n",
            "  list(['batter', 'hit', 'ball', 'att', 'park', 'pacific', 'ocean'])\n",
            "  list(['pitcher', 'threw', 'ball', 'dugout'])]] \n",
            "\n",
            "docs minus stopwords: \n",
            " [['dog ran steps entered owners room check owner room'\n",
            "  'name thomson comer commander machine learning program lambda school'\n",
            "  'creating curriculum machine learning program teaching fulltime machine learning program'\n",
            "  'machine learning one favorite subjects'\n",
            "  'excited taking machine learning class lambda school starting april'\n",
            "  'machine learning program kickoff lambda school'\n",
            "  'batter hit ball att park pacific ocean' 'pitcher threw ball dugout']] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JjAxAZXTt45S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #2: Use the document to create a dictionary - a dictionary maps every word to a number**"
      ]
    },
    {
      "metadata": {
        "id": "p9w477lVuIfs",
        "colab_type": "code",
        "outputId": "d29d9622-39ec-43b1-b686-3ae49d5be877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(tokens)\n",
        "dictionary2 = corpora.Dictionary(nontokens)\n",
        "print('tokens dictionary: ', dictionary.token2id)\n",
        "print(len(dictionary), ' token words')\n",
        "print(len(dictionary2), ' token words minus stopwords')\n",
        "print('stopwords:', list(set([item for sublist in tokens for item in sublist]) - set([item for sublist in nontokens for item in sublist])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokens dictionary:  {'and': 0, 'check': 1, 'dog': 2, 'entered': 3, 'if': 4, 'in': 5, 'owner': 6, 'owners': 7, 'ran': 8, 'room': 9, 'steps': 10, 'the': 11, 'to': 12, 'up': 13, 'was': 14, 'at': 15, 'comer': 16, 'commander': 17, 'is': 18, 'lambda': 19, 'learning': 20, 'machine': 21, 'my': 22, 'name': 23, 'of': 24, 'program': 25, 'school': 26, 'thomson': 27, 'am': 28, 'be': 29, 'creating': 30, 'curriculum': 31, 'for': 32, 'fulltime': 33, 'i': 34, 'teaching': 35, 'will': 36, 'favorite': 37, 'one': 38, 'subjects': 39, 'about': 40, 'april': 41, 'class': 42, 'excited': 43, 'starting': 44, 'taking': 45, 'does': 46, 'kickoff': 47, 'when': 48, 'att': 49, 'ball': 50, 'batter': 51, 'hit': 52, 'into': 53, 'ocean': 54, 'off': 55, 'out': 56, 'pacific': 57, 'park': 58, 'dugout': 59, 'pitcher': 60, 'threw': 61}\n",
            "62  token words\n",
            "40  token words minus stopwords\n",
            "stopwords: ['in', 'at', 'off', 'am', 'if', 'will', 'is', 'does', 'into', 'be', 'my', 'i', 'to', 'and', 'for', 'was', 'the', 'when', 'about', 'of', 'out', 'up']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Q1E2EONvRlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #3: Convert the list of tokens from the document (created above in Step 1) into a bag of words. The bag of words highlights the term frequency i.e. each element in the bag of words is the index of the word in the dictionary and the # of times it occurs**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zfIFJT0Lv0an",
        "colab_type": "code",
        "outputId": "469798ec-6159-44ca-8249-deabeb1a5824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(text) for text in tokens]\n",
        "corpus2 = [dictionary.doc2bow(text) for text in nontokens]\n",
        "print('corpus: \\n', np.matrix(corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus: \n",
            " [[list([(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 5), (12, 1), (13, 1), (14, 1)])\n",
            "  list([(11, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)])\n",
            "  list([(0, 1), (11, 3), (20, 2), (21, 2), (25, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)])\n",
            "  list([(18, 1), (20, 1), (21, 1), (22, 1), (24, 1), (37, 1), (38, 1), (39, 1)])\n",
            "  list([(5, 1), (11, 2), (15, 1), (19, 1), (20, 1), (21, 1), (26, 1), (28, 1), (34, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)])\n",
            "  list([(11, 1), (15, 1), (19, 1), (20, 1), (21, 1), (25, 1), (26, 1), (46, 1), (47, 1), (48, 1)])\n",
            "  list([(11, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1)])\n",
            "  list([(11, 3), (50, 1), (53, 1), (59, 1), (60, 1), (61, 1)])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f9rsM5d1xH2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #4:  Use the \"*Gensim*\" library to create a TF-IDF module for the bag of words**"
      ]
    },
    {
      "metadata": {
        "id": "ifduox8us8f_",
        "colab_type": "code",
        "outputId": "de8e5695-b1ce-48b0-caae-6e24dbfdb288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "tfidf = TfidfModel(corpus);\n",
        "tfidf2 = TfidfModel(corpus2);\n",
        "print(np.matrix([tfidf[i] for i in corpus]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[list([(0, 0.16670846296747688), (1, 0.2500626944512153), (2, 0.2500626944512153), (3, 0.2500626944512153), (4, 0.2500626944512153), (5, 0.16670846296747688), (6, 0.2500626944512153), (7, 0.2500626944512153), (8, 0.2500626944512153), (9, 0.5001253889024306), (10, 0.2500626944512153), (11, 0.08028891210506647), (12, 0.2500626944512153), (13, 0.2500626944512153), (14, 0.2500626944512153)])\n",
            "  list([(11, 0.025524077576628612), (15, 0.18748222010754678), (16, 0.39747827220782794), (17, 0.39747827220782794), (18, 0.26498551480521865), (19, 0.18748222010754678), (20, 0.08983961642561385), (21, 0.08983961642561385), (22, 0.26498551480521865), (23, 0.39747827220782794), (24, 0.26498551480521865), (25, 0.18748222010754678), (26, 0.18748222010754678), (27, 0.39747827220782794)])\n",
            "  list([(0, 0.21439591157214022), (11, 0.06195347564301893), (20, 0.14537584420808175), (21, 0.14537584420808175), (25, 0.3033782545666494), (28, 0.21439591157214022), (29, 0.32159386735821033), (30, 0.32159386735821033), (31, 0.32159386735821033), (32, 0.32159386735821033), (33, 0.32159386735821033), (34, 0.21439591157214022), (35, 0.32159386735821033), (36, 0.32159386735821033)])\n",
            "  list([(18, 0.31654620145259826), (20, 0.10732054293756607), (21, 0.10732054293756607), (22, 0.31654620145259826), (24, 0.31654620145259826), (37, 0.4748193021788974), (38, 0.4748193021788974), (39, 0.4748193021788974)])\n",
            "  list([(5, 0.23396213138068614), (11, 0.04507165303540133), (15, 0.1655325946574372), (19, 0.1655325946574372), (20, 0.07932157407475755), (21, 0.07932157407475755), (26, 0.1655325946574372), (28, 0.23396213138068614), (34, 0.23396213138068614), (40, 0.3509431970710292), (41, 0.3509431970710292), (42, 0.3509431970710292), (43, 0.3509431970710292), (44, 0.3509431970710292), (45, 0.3509431970710292)])\n",
            "  list([(11, 0.03212268578019072), (15, 0.2359510320326633), (19, 0.2359510320326633), (20, 0.11306538935202697), (21, 0.11306538935202697), (25, 0.2359510320326633), (26, 0.2359510320326633), (46, 0.5002362809881282), (47, 0.5002362809881282), (48, 0.5002362809881282)])\n",
            "  list([(11, 0.06448065662695637), (49, 0.33471219361356946), (50, 0.2231414624090463), (51, 0.33471219361356946), (52, 0.33471219361356946), (53, 0.2231414624090463), (54, 0.33471219361356946), (55, 0.33471219361356946), (56, 0.33471219361356946), (57, 0.33471219361356946), (58, 0.33471219361356946)])\n",
            "  list([(11, 0.09722606600948237), (50, 0.33646007482753554), (53, 0.33646007482753554), (59, 0.5046901122413033), (60, 0.5046901122413033), (61, 0.5046901122413033)])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yfmg609ue5B0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #5: a) Output the 5th document, b) Output the bag of words for the fifth document i.e. term frequency, c) Review the Inter Document Frequency (IDF) for each term in the bag of words for the 5th document**"
      ]
    },
    {
      "metadata": {
        "id": "C330my8hfokU",
        "colab_type": "code",
        "outputId": "8e5d8e43-db49-4e17-93eb-03439b2f9316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "n = 0\n",
        "\n",
        "print(raw_documents[n])\n",
        "print(tokens[n])\n",
        "print(corpus[n])\n",
        "print(tfidf[corpus[n]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dog ran up the steps and entered the owner's room to check if the owner was in the room.\n",
            "['the', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'the', 'owners', 'room', 'to', 'check', 'if', 'the', 'owner', 'was', 'in', 'the', 'room']\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 5), (12, 1), (13, 1), (14, 1)]\n",
            "[(0, 0.16670846296747688), (1, 0.2500626944512153), (2, 0.2500626944512153), (3, 0.2500626944512153), (4, 0.2500626944512153), (5, 0.16670846296747688), (6, 0.2500626944512153), (7, 0.2500626944512153), (8, 0.2500626944512153), (9, 0.5001253889024306), (10, 0.2500626944512153), (11, 0.08028891210506647), (12, 0.2500626944512153), (13, 0.2500626944512153), (14, 0.2500626944512153)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hP1HRC-gUEf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #6: Determine document similarity** -  Identify the most similar document and the least similar document to the body of text below.\n",
        "\n",
        "*Good Reference for review*: https://radimrehurek.com/gensim/similarities/docsim.html"
      ]
    },
    {
      "metadata": {
        "id": "VVyDyZ9ThkGI",
        "colab_type": "code",
        "outputId": "cca7cf31-3ebb-4291-a82c-5f1be00e33e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "t = \"\\n\\033[1mMachine Learning at Lambda school is awesome\\033[0m\"\n",
        "print(t, '\\n')\n",
        "\n",
        "def cosine_sim(text1, text2):\n",
        "    v = vectorizer.fit_transform([text1, text2])\n",
        "    return ((v * v.T).A)[0,1]\n",
        "\n",
        "print('\\033[4mSimilarities with stopwords\\033[0m:')\n",
        "for i in raw_documents: print(round(cosine_sim(i, t),3), ': ', i)\n",
        "  \n",
        "t2 = \"\\n\\033[1mMachine Learning Lambda school awesome\\033[0m\"\n",
        "print(t2, '\\n')\n",
        "\n",
        "print('\\033[4mSimilarities without stopwords\\033[0m:')\n",
        "for i in docs2: print(round(cosine_sim(i, t2),3), ': ', i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mMachine Learning at Lambda school is awesome\u001b[0m \n",
            "\n",
            "\u001b[4mSimilarities with stopwords\u001b[0m:\n",
            "0.0 :  The dog ran up the steps and entered the owner's room to check if the owner was in the room.\n",
            "0.317 :  My name is Thomson Comer, commander of the Machine Learning program at Lambda school.\n",
            "0.069 :  I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.\n",
            "0.144 :  Machine Learning is one of my favorite subjects.\n",
            "0.213 :  I am excited about taking the Machine Learning class at the Lambda school starting in April.\n",
            "0.275 :  When does the Machine Learning program kick-off at Lambda school?\n",
            "0.043 :  The batter hit the ball out off AT&T park into the pacific ocean.\n",
            "0.0 :  The pitcher threw the ball into the dug-out.\n",
            "\n",
            "\u001b[1mMachine Learning Lambda school awesome\u001b[0m \n",
            "\n",
            "\u001b[4mSimilarities without stopwords\u001b[0m:\n",
            "0.0 :  dog ran steps entered owners room check owner room\n",
            "0.261 :  name thomson comer commander machine learning program lambda school\n",
            "0.115 :  creating curriculum machine learning program teaching fulltime machine learning program\n",
            "0.102 :  machine learning one favorite subjects\n",
            "0.261 :  excited taking machine learning class lambda school starting april\n",
            "0.336 :  machine learning program kickoff lambda school\n",
            "0.0 :  batter hit ball att park pacific ocean\n",
            "0.0 :  pitcher threw ball dugout\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}