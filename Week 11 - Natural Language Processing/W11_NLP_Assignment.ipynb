{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marina - NLP - Assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "B513-nWV9iSm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Assignment: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "Xo5zIUFW9tLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will work with a data set that contains restaurant reviews. You will use a Naive Bayes model to classify the reviews (positive or negative) based on the words in the review.  The main objective of this assignment is gauge the performance of a Naive Bayes model by using a confusion matrix; however in order to ascertain the efficiacy of the model, you will have to first train the Naive Bayes model with a portion (i.e. 70%) of the underlying data set and then test it against the remainder of the data set . Before you can train the model, you will have to go through a sequence of steps to get the data ready for training the model.\n",
        "\n",
        "Steps you may need to perform:\n",
        "\n",
        "**1) **Read in the list of restaurant reviews\n",
        "\n",
        "**2)** Convert the reviews into a list of tokens\n",
        "\n",
        "**3) **You will most likely have to eliminate stop words\n",
        "\n",
        "**4)** You may have to utilize stemming or lemmatization to determine the base form of the words\n",
        "\n",
        "**5) **You will have to vectorize the data (i.e. construct a document term/word matix) wherein select words from the reviews will constitute the columns of the matrix and the individual reviews will be part of the rows of the matrix\n",
        "\n",
        "**6) ** Create 'Train' and 'Test' data sets (i.e. 70% of the underlying data set will constitute the training set and 30% of the underlying data set will constitute the test set)\n",
        "\n",
        "**7)** Train a Naive Bayes model on the Train data set and test it against the test data set\n",
        "\n",
        "**8) **Construct a confusion matirx to gauge the performance of the model\n",
        "\n",
        "**Dataset**: https://www.dropbox.com/s/yl5r7kx9nq15gmi/Restaurant_Reviews.tsv?raw=1\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-cP6WzuqNoK0",
        "colab_type": "code",
        "outputId": "4e6510b6-8c27-4184-840e-a0b26960e45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4843
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk\n",
        "!pip install regex\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "# Inaugural is one of the data packages included within NLTK\n",
        "\n",
        "# Import the \"inaugural\" data package\n",
        "from nltk.corpus import inaugural"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 6.3MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Running setup.py bdist_wheel for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.3\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/ca/93cad3699d8022a29493e9cf180f7691ead38da64eae819f9c1ae186ba56/regex-2018.06.09.tar.gz (632kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 5.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: regex\n",
            "  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/3f/31/e7/ad6a5b9d818317808538449fd68cfbacb97609b9fa3b716cd1\n",
            "Successfully built regex\n",
            "Installing collected packages: regex\n",
            "Successfully installed regex-2018.6.9\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kTPLbX3UMqPb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import PorterStemmer, LancasterStemmer, word_tokenize\n",
        "\n",
        "df = pd.read_table('https://www.dropbox.com/s/yl5r7kx9nq15gmi/Restaurant_Reviews.tsv?raw=1')\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "# Remove punctuation\n",
        "df['clean'] = df['Review'].apply(lambda x: re.sub(r'[^\\w\\s]','',x).lower())\n",
        "en_stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
        "keep = ['not', 'no']\n",
        "en_stopwords = [i for i in en_stopwords if i not in keep]\n",
        "\n",
        "# Tokenize the sentence\n",
        "df['tokens']  = df['clean'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Tokens without stopwords\n",
        "df['nontokens'] = df['tokens'].apply(lambda x:' '.join([i for i in x if i not in en_stopwords]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lvoVT1aM2pA",
        "colab_type": "code",
        "outputId": "ee4918c7-2fc5-400b-bb58-6afc5768166a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vfit = vectorizer.fit_transform(df['nontokens'])\n",
        "vfit = vfit.A\n",
        "print(vfit.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 1958)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ut9335C3k_p7",
        "colab_type": "code",
        "outputId": "aa6588da-7f6c-4ad0-b201-85f4ce9bbd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "cell_type": "code",
      "source": [
        "# !pip install gensim\n",
        "import gensim\n",
        "\n",
        "from gensim.models.fasttext import FastText\n",
        "model = FastText(df['tokens'], min_count=1)\n",
        "model['good']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.17617202, -0.02403634,  0.3577154 , -0.0887966 , -0.00076755,\n",
              "       -0.09830803, -0.21637231, -0.09890354, -0.04458106,  0.22272782,\n",
              "       -0.10851035, -0.04727236,  0.2505561 , -0.25612572,  0.21001679,\n",
              "        0.07326671, -0.16497612, -0.09847432,  0.16445106, -0.02900513,\n",
              "       -0.12236547,  0.09080641,  0.1500903 , -0.27190936, -0.11710636,\n",
              "       -0.42419073, -0.17777662, -0.04790103, -0.06882516,  0.07657215,\n",
              "       -0.174973  , -0.21405645, -0.09907134,  0.0373577 ,  0.05012957,\n",
              "       -0.03536616, -0.17542887, -0.25021127,  0.03585653,  0.2362842 ,\n",
              "       -0.02786737, -0.06347536, -0.27179083,  0.17428143, -0.04533366,\n",
              "       -0.07112062, -0.21571207,  0.11335783, -0.08055928,  0.17356868,\n",
              "       -0.00181954,  0.03059168, -0.0147906 , -0.09463813,  0.13151124,\n",
              "       -0.09264892,  0.04354956,  0.14521304,  0.45219973, -0.27504757,\n",
              "       -0.09550574,  0.00325185, -0.09354495, -0.01502891,  0.1675005 ,\n",
              "        0.00526447, -0.06332537, -0.36587086, -0.21194302, -0.11906602,\n",
              "        0.06970741,  0.16166188, -0.00882489,  0.217698  , -0.1481652 ,\n",
              "       -0.25504026,  0.22878283, -0.01014243,  0.16181777,  0.30539134,\n",
              "        0.04166592,  0.15460037, -0.27497104, -0.15109886,  0.0615996 ,\n",
              "        0.10252197,  0.11082435,  0.02974173,  0.11843888, -0.11478856,\n",
              "       -0.06122696,  0.00753369, -0.11434812, -0.11978107,  0.0610557 ,\n",
              "       -0.15656433, -0.14304878, -0.2908942 , -0.01310644,  0.2606591 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "wMsSHcGaN6DM",
        "colab_type": "code",
        "outputId": "8ac3d122-c317-4551-bf51-29adec7203e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "from operator import itemgetter\n",
        "from collections import Counter\n",
        "\n",
        "# Count how many times each work appears\n",
        "count = Counter(\" \".join(df['nontokens']).split(\" \")).items()\n",
        "sorted_count = sorted(count, key=itemgetter(1))\n",
        "sorted_count.reverse()\n",
        "print(sorted_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('food', 124), ('not', 118), ('place', 106), ('good', 95), ('service', 83), ('great', 70), ('back', 61), ('like', 46), ('go', 43), ('time', 42), ('really', 36), ('best', 30), ('dont', 28), ('ever', 28), ('would', 28), ('also', 27), ('one', 26), ('friendly', 26), ('never', 26), ('nice', 25), ('restaurant', 25), ('no', 25), ('delicious', 23), ('amazing', 23), ('im', 21), ('vegas', 21), ('experience', 20), ('ive', 20), ('came', 20), ('wont', 19), ('disappointed', 19), ('even', 19), ('love', 19), ('minutes', 19), ('us', 19), ('eat', 19), ('get', 19), ('staff', 19), ('pretty', 19), ('going', 18), ('well', 18), ('got', 18), ('definitely', 18), ('much', 18), ('bad', 18), ('first', 17), ('chicken', 17), ('made', 17), ('better', 16), ('think', 16), ('say', 16), ('could', 16), ('pizza', 15), ('always', 15), ('stars', 15), ('salad', 15), ('menu', 15), ('steak', 14), ('wait', 14), ('way', 14), ('ordered', 14), ('worst', 14), ('fresh', 14), ('flavor', 13), ('wasnt', 13), ('sushi', 13), ('times', 13), ('server', 13), ('quality', 13), ('taste', 13), ('didnt', 13), ('want', 13), ('awesome', 12), ('night', 12), ('fantastic', 12), ('went', 12), ('enough', 12), ('burger', 12), ('recommend', 11), ('order', 11), ('come', 11), ('know', 11), ('next', 11), ('meal', 11), ('bland', 11), ('cant', 11), ('buffet', 11), ('feel', 11), ('slow', 11), ('still', 11), ('tasty', 11), ('perfect', 10), ('terrible', 10), ('probably', 10), ('waited', 10), ('excellent', 10), ('atmosphere', 10), ('another', 10), ('everything', 10), ('coming', 10), ('sauce', 10), ('worth', 10), ('little', 10), ('prices', 10), ('selection', 10), ('loved', 10), ('cold', 9), ('soon', 9), ('hot', 9), ('day', 9), ('clean', 9), ('many', 9), ('dishes', 9), ('breakfast', 9), ('meat', 9), ('lunch', 9), ('give', 9), ('sandwich', 9), ('people', 9), ('table', 9), ('every', 9), ('fries', 9), ('tasted', 8), ('happy', 8), ('two', 8), ('make', 8), ('felt', 8), ('take', 8), ('impressed', 8), ('since', 8), ('rude', 8), ('spot', 8), ('said', 8), ('bit', 8), ('spicy', 8), ('dining', 8), ('try', 8), ('quite', 8), ('dish', 8), ('2', 8), ('5', 8), ('absolutely', 8), ('right', 8), ('thing', 8), ('overall', 8), ('beer', 8), ('around', 8), ('took', 8), ('waitress', 8), ('sure', 8), ('overpriced', 8), ('getting', 8), ('super', 7), ('bar', 7), ('family', 7), ('friends', 7), ('far', 7), ('chips', 7), ('area', 7), ('seriously', 7), ('ambiance', 7), ('ill', 7), ('price', 7), ('town', 7), ('eating', 7), ('nothing', 7), ('wonderful', 7), ('inside', 7), ('side', 7), ('cooked', 7), ('attentive', 7), ('waiter', 7), ('lot', 7), ('found', 7), ('deal', 7), ('tried', 7), ('dinner', 6), ('authentic', 6), ('twice', 6), ('money', 6), ('special', 6), ('either', 6), ('see', 6), ('small', 6), ('last', 6), ('must', 6), ('done', 6), ('management', 6), ('mediocre', 6), ('served', 6), ('check', 6), ('potato', 6), ('enjoy', 6), ('horrible', 6), ('left', 6), ('hard', 6), ('tender', 6), ('shrimp', 6), ('poor', 6), ('warm', 6), ('hour', 6), ('tell', 6), ('pho', 6), ('barely', 5), ('long', 5), ('bring', 5), ('new', 5), ('anytime', 5), ('enjoyed', 5), ('reasonable', 5), ('expect', 5), ('fried', 5), ('wrong', 5), ('fast', 5), ('owners', 5), ('waiting', 5), ('things', 5), ('home', 5), ('bread', 5), ('ice', 5), ('sat', 5), ('zero', 5), ('bacon', 5), ('full', 5), ('avoid', 5), ('id', 5), ('seated', 5), ('extremely', 5), ('return', 5), ('waste', 5), ('hit', 5), ('away', 5), ('thought', 5), ('places', 5), ('eaten', 5), ('thai', 5), ('real', 5), ('disappointing', 5), ('old', 5), ('sweet', 5), ('customer', 5), ('30', 5), ('fish', 5), ('10', 5), ('tasteless', 5), ('pasta', 5), ('seafood', 5), ('servers', 5), ('huge', 5), ('talk', 5), ('outside', 5), ('beef', 5), ('dessert', 5), ('sick', 5), ('portions', 5), ('quick', 5), ('tables', 5), ('asked', 5), ('kept', 5), ('may', 5), ('average', 4), ('dry', 4), ('thin', 4), ('manager', 4), ('40', 4), ('needs', 4), ('disappointment', 4), ('options', 4), ('soup', 4), ('large', 4), ('vegetables', 4), ('dirty', 4), ('1', 4), ('gave', 4), ('location', 4), ('tea', 4), ('job', 4), ('close', 4), ('possible', 4), ('fun', 4), ('folks', 4), ('someone', 4), ('business', 4), ('though', 4), ('drive', 4), ('live', 4), ('wings', 4), ('sucked', 4), ('incredible', 4), ('something', 4), ('20', 4), ('sad', 4), ('phoenix', 4), ('unfortunately', 4), ('piece', 4), ('bay', 4), ('pay', 4), ('thumbs', 4), ('3', 4), ('considering', 4), ('drink', 4), ('fact', 4), ('cream', 4), ('burgers', 4), ('pork', 4), ('leave', 4), ('steaks', 4), ('kind', 4), ('trip', 4), ('wine', 4), ('beat', 4), ('helpful', 4), ('yummy', 4), ('everyone', 4), ('ask', 4), ('today', 4), ('rice', 4), ('drinks', 4), ('least', 4), ('heart', 4), ('husband', 4), ('rare', 4), ('establishment', 4), ('visit', 4), ('elsewhere', 4), ('look', 4), ('salmon', 4), ('running', 4), ('tacos', 4), ('highly', 4), ('damn', 4), ('lacked', 3), ('awful', 3), ('salsa', 3), ('style', 3), ('seating', 3), ('edible', 3), ('vibe', 3), ('list', 3), ('half', 3), ('ok', 3), ('wanted', 3), ('setting', 3), ('believe', 3), ('maybe', 3), ('room', 3), ('boyfriend', 3), ('desserts', 3), ('need', 3), ('gone', 3), ('homemade', 3), ('meals', 3), ('worse', 3), ('bill', 3), ('rather', 3), ('35', 3), ('big', 3), ('regular', 3), ('total', 3), ('kids', 3), ('told', 3), ('beans', 3), ('green', 3), ('stomach', 3), ('deserves', 3), ('find', 3), ('else', 3), ('cool', 3), ('door', 3), ('bathroom', 3), ('equally', 3), ('pleased', 3), ('pleasant', 3), ('especially', 3), ('favorite', 3), ('house', 3), ('high', 3), ('12', 3), ('stay', 3), ('review', 3), ('please', 3), ('star', 3), ('pulled', 3), ('pizzas', 3), ('outstanding', 3), ('patio', 3), ('8', 3), ('generous', 3), ('received', 3), ('flavorful', 3), ('point', 3), ('couldnt', 3), ('yet', 3), ('couple', 3), ('chef', 3), ('isnt', 3), ('none', 3), ('course', 3), ('lobster', 3), ('unless', 3), ('soggy', 3), ('white', 3), ('brunch', 3), ('hope', 3), ('insulted', 3), ('dog', 3), ('egg', 3), ('butter', 3), ('decor', 3), ('interesting', 3), ('lacking', 3), ('healthy', 3), ('reviews', 3), ('vegetarian', 3), ('used', 3), ('literally', 3), ('treated', 3), ('immediately', 3), ('ago', 3), ('years', 3), ('several', 3), ('different', 3), ('stale', 3), ('watched', 3), ('ate', 3), ('checked', 3), ('empty', 3), ('restaurants', 3), ('subway', 3), ('cheese', 3), ('melt', 3), ('delish', 3), ('strip', 3), ('wouldnt', 3), ('fine', 3), ('liked', 3), ('although', 3), ('busy', 3), ('honest', 3), ('music', 3), ('ambience', 3), ('wife', 3), ('cafe', 3), ('quickly', 3), ('arrived', 3), ('tip', 3), ('youre', 3), ('mouth', 3), ('party', 3), ('oh', 3), ('rolls', 3), ('walked', 3), ('guess', 3), ('expected', 3), ('chewy', 3), ('salt', 3), ('second', 3), ('water', 3), ('perfectly', 3), ('amount', 3), ('duck', 3), ('hummus', 3), ('pita', 3), ('dressing', 3), ('greek', 3), ('frozen', 3), ('sides', 3), ('moist', 3), ('sucks', 3), ('sashimi', 3), ('totally', 3), ('4', 3), ('wall', 3), ('brought', 3), ('beautiful', 3), ('recommended', 3), ('care', 3), ('potatoes', 3), ('recommendation', 3), ('nasty', 3), ('texture', 3), ('wow', 3), ('seemed', 2), ('undercooked', 2), ('lost', 2), ('spend', 2), ('three', 2), ('arent', 2), ('started', 2), ('heat', 2), ('flat', 2), ('fell', 2), ('tribute', 2), ('crazy', 2), ('nachos', 2), ('flavorless', 2), ('offers', 2), ('bother', 2), ('definately', 2), ('mid', 2), ('crowd', 2), ('complain', 2), ('anything', 2), ('paper', 2), ('chinese', 2), ('fly', 2), ('simple', 2), ('orders', 2), ('longer', 2), ('serving', 2), ('needless', 2), ('plate', 2), ('fail', 2), ('work', 2), ('owner', 2), ('staying', 2), ('legit', 2), ('italian', 2), ('hands', 2), ('promise', 2), ('guy', 2), ('focused', 2), ('means', 2), ('grilled', 2), ('satisfying', 2), ('fairly', 2), ('sitting', 2), ('color', 2), ('presentation', 2), ('cheap', 2), ('rated', 2), ('fan', 2), ('recently', 2), ('helped', 2), ('stop', 2), ('disgusting', 2), ('baby', 2), ('vinegrette', 2), ('tapas', 2), ('however', 2), ('recent', 2), ('actually', 2), ('entrees', 2), ('choose', 2), ('whole', 2), ('lovely', 2), ('filling', 2), ('sliced', 2), ('legs', 2), ('crab', 2), ('group', 2), ('flower', 2), ('disappoint', 2), ('margaritas', 2), ('acknowledged', 2), ('paid', 2), ('tots', 2), ('driest', 2), ('combination', 2), ('week', 2), ('anyway', 2), ('cow', 2), ('biscuits', 2), ('nicest', 2), ('mistake', 2), ('weve', 2), ('end', 2), ('professional', 2), ('become', 2), ('completely', 2), ('trying', 2), ('assure', 2), ('pop', 2), ('simply', 2), ('impeccable', 2), ('perfection', 2), ('lots', 2), ('mall', 2), ('stuffed', 2), ('anyone', 2), ('front', 2), ('car', 2), ('happened', 2), ('part', 2), ('fry', 2), ('eggplant', 2), ('comfortable', 2), ('lukewarm', 2), ('life', 2), ('evening', 2), ('treat', 2), ('eggs', 2), ('pancakes', 2), ('finally', 2), ('ten', 2), ('multiple', 2), ('oven', 2), ('brick', 2), ('feels', 2), ('omg', 2), ('bites', 2), ('mom', 2), ('seen', 2), ('serve', 2), ('black', 2), ('doubt', 2), ('without', 2), ('pricing', 2), ('fare', 2), ('joke', 2), ('basically', 2), ('hand', 2), ('boy', 2), ('toasted', 2), ('delightful', 2), ('summer', 2), ('stir', 2), ('third', 2), ('soooo', 2), ('convenient', 2), ('neighborhood', 2), ('later', 2), ('hours', 2), ('hate', 2), ('100', 2), ('salads', 2), ('bachi', 2), ('taco', 2), ('boba', 2), ('world', 2), ('coffee', 2), ('par', 2), ('bucks', 2), ('plus', 2), ('tap', 2), ('sticks', 2), ('complaints', 2), ('similar', 2), ('creamy', 2), ('set', 2), ('dark', 2), ('building', 2), ('almost', 2), ('stayed', 2), ('honestly', 2), ('yum', 2), ('single', 2), ('cheeseburger', 2), ('double', 2), ('dealing', 2), ('cook', 2), ('needed', 2), ('filet', 2), ('bisque', 2), ('mind', 2), ('shower', 2), ('friend', 2), ('given', 2), ('bug', 2), ('puree', 2), ('gold', 2), ('disrespected', 2), ('bowl', 2), ('valley', 2), ('feeling', 2), ('gyros', 2), ('watch', 2), ('station', 2), ('dirt', 2), ('looked', 2), ('decided', 2), ('charcoal', 2), ('overcooked', 2), ('ladies', 2), ('looking', 2), ('priced', 2), ('reasonably', 2), ('bakery', 2), ('cut', 2), ('joint', 2), ('greeted', 2), ('pace', 2), ('decorated', 2), ('bathrooms', 2), ('subpar', 2), ('curry', 2), ('rarely', 2), ('dine', 2), ('bars', 2), ('selections', 2), ('bagels', 2), ('tuna', 2), ('wrap', 2), ('belly', 2), ('word', 2), ('youd', 2), ('boot', 2), ('indian', 2), ('preparing', 2), ('gross', 2), ('passed', 2), ('lovers', 2), ('playing', 2), ('bartender', 2), ('extra', 2), ('added', 2), ('marrow', 2), ('garlic', 2), ('roasted', 2), ('serves', 2), ('lady', 2), ('bye', 2), ('die', 2), ('year', 2), ('six', 2), ('sugary', 2), ('roast', 2), ('others', 2), ('grease', 2), ('break', 2), ('person', 2), ('underwhelming', 2), ('company', 2), ('opportunity', 2), ('wasting', 2), ('seasoned', 2), ('value', 2), ('use', 2), ('suck', 2), ('known', 2), ('min', 2), ('meh', 2), ('beyond', 2), ('finish', 2), ('batter', 2), ('heard', 2), ('glad', 2), ('appetizers', 2), ('refill', 2), ('ripped', 2), ('scallop', 2), ('excuse', 2), ('downtown', 2), ('grill', 2), ('attack', 2), ('portion', 2), ('customers', 2), ('towards', 2), ('attitudes', 2), ('realized', 2), ('char', 2), ('behind', 2), ('note', 2), ('grossed', 2), ('judge', 2), ('delight', 2), ('inexpensive', 2), ('favor', 2), ('familiar', 2), ('bite', 2), ('seems', 2), ('decent', 2), ('overwhelmed', 2), ('mexican', 2), ('stuff', 2), ('red', 2), ('thats', 2), ('cute', 2), ('let', 2), ('hair', 2), ('human', 2), ('ended', 2), ('cashier', 2), ('touch', 2), ('late', 2), ('stopped', 2), ('crust', 2), ('drawing', 1), ('wound', 1), ('poured', 1), ('wasted', 1), ('hadnt', 1), ('ninja', 1), ('instantly', 1), ('appetite', 1), ('caterpillar', 1), ('crusty', 1), ('dried', 1), ('refried', 1), ('mirage', 1), ('closed', 1), ('hasnt', 1), ('impressive', 1), ('brushfire', 1), ('mile', 1), ('unwrapped', 1), ('sub', 1), ('bigger', 1), ('1199', 1), ('charge', 1), ('problem', 1), ('flop', 1), ('ha', 1), ('movies', 1), ('obviously', 1), ('brownish', 1), ('tops', 1), ('circumstances', 1), ('vomited', 1), ('owned', 1), ('cause', 1), ('putting', 1), ('prettyoff', 1), ('heimer', 1), ('correction', 1), ('disapppointment', 1), ('fireball', 1), ('shots', 1), ('yaall', 1), ('services', 1), ('gotten', 1), ('hell', 1), ('hamburger', 1), ('avoided', 1), ('del', 1), ('herewhat', 1), ('rave', 1), ('placed', 1), ('blame', 1), ('cart', 1), ('ala', 1), ('combo', 1), ('solidify', 1), ('smoke', 1), ('deuchebaggery', 1), ('profound', 1), ('insults', 1), ('carbs', 1), ('drinking', 1), ('binge', 1), ('fill', 1), ('reason', 1), ('apologize', 1), ('ensued', 1), ('neither', 1), ('spotty', 1), ('con', 1), ('garden', 1), ('indoor', 1), ('douchey', 1), ('workers', 1), ('watered', 1), ('sause', 1), ('dipping', 1), ('ranch', 1), ('chipolte', 1), ('cashew', 1), ('imagination', 1), ('stretch', 1), ('reservation', 1), ('hurry', 1), ('trippy', 1), ('neat', 1), ('brisket', 1), ('ridiculous', 1), ('glass', 1), ('sangria', 1), ('saying', 1), ('waaaaaayyyyyyyyyy', 1), ('downright', 1), ('dennys', 1), ('itdefinitely', 1), ('scene', 1), ('ones', 1), ('forever', 1), ('fondue', 1), ('calamari', 1), ('ri', 1), ('degree', 1), ('teamwork', 1), ('caring', 1), ('entire', 1), ('biggest', 1), ('remember', 1), ('eve', 1), ('christmas', 1), ('hilarious', 1), ('yay', 1), ('thinking', 1), ('batch', 1), ('poisoning', 1), ('postinos', 1), ('puréed', 1), ('guacamole', 1), ('concept', 1), ('understand', 1), ('properly', 1), ('steakhouse', 1), ('call', 1), ('heads', 1), ('chickens', 1), ('employees', 1), ('unexperienced', 1), ('fav', 1), ('returning', 1), ('sals', 1), ('bought', 1), ('correct', 1), ('weak', 1), ('anticipated', 1), ('bellagio', 1), ('mmmm', 1), ('pats', 1), ('occasional', 1), ('patron', 1), ('loyal', 1), ('unprofessional', 1), ('strawberry', 1), ('mushroom', 1), ('mellow', 1), ('concern', 1), ('highlight', 1), ('beers', 1), ('rotating', 1), ('mediterranean', 1), ('peanuts', 1), ('contain', 1), ('clue', 1), ('warnings', 1), ('allergy', 1), ('latte', 1), ('chai', 1), ('killer', 1), ('anymore', 1), ('refused', 1), ('bloodiest', 1), ('medium', 1), ('occasions', 1), ('meatballs', 1), ('tomato', 1), ('albondigas', 1), ('wire', 1), ('inch', 1), ('doughy', 1), ('dude', 1), ('japanese', 1), ('ownerchef', 1), ('store', 1), ('grocery', 1), ('hardly', 1), ('weird', 1), ('unique', 1), ('highlighted', 1), ('costcos', 1), ('besides', 1), ('editing', 1), ('class', 1), ('cooking', 1), ('college', 1), ('light', 1), ('approval', 1), ('seal', 1), ('gets', 1), ('sandwiches', 1), ('plethora', 1), ('pleasure', 1), ('oil', 1), ('deep', 1), ('gloveseverything', 1), ('bare', 1), ('prepare', 1), ('tempi', 1), ('foodservice', 1), ('pricey', 1), ('wines', 1), ('appealing', 1), ('rancheros', 1), ('huevos', 1), ('fella', 1), ('coconut', 1), ('hated', 1), ('biscuit', 1), ('otherwise', 1), ('wash', 1), ('polite', 1), ('tolerance', 1), ('low', 1), ('foods', 1), ('version', 1), ('qualified', 1), ('disbelief', 1), ('round', 1), ('oysters', 1), ('head', 1), ('caballeros', 1), ('fucking', 1), ('spices', 1), ('read', 1), ('article', 1), ('surprised', 1), ('maria', 1), ('underservices', 1), ('overhip', 1), ('kabuki', 1), ('fair', 1), ('delicate', 1), ('crêpe', 1), ('boxes', 1), ('takeout', 1), ('cramming', 1), ('opposed', 1), ('containers', 1), ('plastic', 1), ('put', 1), ('blown', 1), ('officially', 1), ('peach', 1), ('fruit', 1), ('seasonal', 1), ('packed', 1), ('consistent', 1), ('suggest', 1), ('unwelcome', 1), ('negligent', 1), ('mozzarella', 1), ('lastly', 1), ('employee', 1), ('devine', 1), ('bruschetta', 1), ('hopes', 1), ('goldencrispy', 1), ('surprise', 1), ('pissd', 1), ('youll', 1), ('held', 1), ('event', 1), ('hereas', 1), ('works', 1), ('law', 1), ('brother', 1), ('roll', 1), ('maine', 1), ('classic', 1), ('idea', 1), ('wienerschnitzel', 1), ('looks', 1), ('785', 1), ('paying', 1), ('arrives', 1), ('court', 1), ('lawyers', 1), ('juries', 1), ('crowds', 1), ('words', 1), ('easily', 1), ('chains', 1), ('tepid', 1), ('describing', 1), ('colder', 1), ('case', 1), ('likes', 1), ('email', 1), ('via', 1), ('club', 1), ('join', 1), ('satifying', 1), ('veal', 1), ('girlfriends', 1), ('sewer', 1), ('market', 1), ('smells', 1), ('restaraunt', 1), ('nutshell', 1), ('bartenders', 1), ('flair', 1), ('hunan', 1), ('traditional', 1), ('sunday', 1), ('soups', 1), ('sour', 1), ('affordable', 1), ('nonfancy', 1), ('lowkey', 1), ('block', 1), ('leather', 1), ('shoe', 1), ('jalapeno', 1), ('arepas', 1), ('30s', 1), ('older', 1), ('main', 1), ('overwhelm', 1), ('spice', 1), ('level', 1), ('thick', 1), ('wontons', 1), ('fabulous', 1), ('fuzzy', 1), ('lil', 1), ('rapidly', 1), ('grow', 1), ('smaller', 1), ('inflate', 1), ('array', 1), ('wide', 1), ('provides', 1), ('extensive', 1), ('guest', 1), ('opened', 1), ('awkwardly', 1), ('begin', 1), ('stood', 1), ('calligraphy', 1), ('speedy', 1), ('airport', 1), ('touched', 1), ('nut', 1), ('pine', 1), ('naan', 1), ('revisiting', 1), ('edinburgh', 1), ('ryans', 1), ('nan', 1), ('han', 1), ('juice', 1), ('apple', 1), ('6', 1), ('larger', 1), ('gratuity', 1), ('tummy', 1), ('dylan', 1), ('thanks', 1), ('compliments', 1), ('deliciously', 1), ('quaint', 1), ('sorely', 1), ('classics', 1), ('hooked', 1), ('specialand', 1), ('elk', 1), ('tonight', 1), ('dough', 1), ('pros', 1), ('reminded', 1), ('reading', 1), ('bellies', 1), ('mouths', 1), ('famous', 1), ('mean', 1), ('cibo', 1), ('recommending', 1), ('refrained', 1), ('paradise', 1), ('industry', 1), ('hospitality', 1), ('2007', 1), ('mostly', 1), ('dripping', 1), ('anyways', 1), ('mortified', 1), ('theyre', 1), ('smoothies', 1), ('pineapple', 1), ('magic', 1), ('mango', 1), ('breeze', 1), ('hawaiian', 1), ('auju', 1), ('pepper', 1), ('sergeant', 1), ('relleno', 1), ('fillet', 1), ('amazingrge', 1), ('soundtrack', 1), ('piano', 1), ('mayo', 1), ('drenched', 1), ('slaw', 1), ('flavors', 1), ('atmosphere1', 1), ('cotta', 1), ('panna', 1), ('talking', 1), ('spends', 1), ('giving', 1), ('plantains', 1), ('gem', 1), ('lover', 1), ('smashburger', 1), ('imagine', 1), ('google', 1), ('nobu', 1), ('pictures', 1), ('rings', 1), ('onion', 1), ('eclectic', 1), ('courteous', 1), ('mgm', 1), ('ganoush', 1), ('baba', 1), ('falafels', 1), ('baklava', 1), ('personally', 1), ('requested', 1), ('songs', 1), ('violinists', 1), ('duo', 1), ('booksomethats', 1), ('screams', 1), ('constructed', 1), ('poorly', 1), ('together', 1), ('sitdown', 1), ('relax', 1), ('plater', 1), ('deliver', 1), ('fails', 1), ('somewhat', 1), ('werent', 1), ('informative', 1), ('insanely', 1), ('macarons', 1), ('moods', 1), ('greatest', 1), ('dead', 1), ('proven', 1), ('madhouse', 1), ('promptly', 1), ('caesar', 1), ('highquality', 1), ('drastically', 1), ('letting', 1), ('shirt', 1), ('blue', 1), ('four', 1), ('boys', 1), ('hostess', 1), ('ignored', 1), ('grandmother', 1), ('noncustomer', 1), ('luck', 1), ('somehow', 1), ('thru', 1), ('seasoning', 1), ('saffron', 1), ('yellow', 1), ('businesses', 1), ('rate', 1), ('despite', 1), ('mary', 1), ('bloody', 1), ('cheek', 1), ('tongue', 1), ('absolute', 1), ('sorry', 1), ('wedges', 1), ('reheated', 1), ('papers', 1), ('freaking', 1), ('soooooo', 1), ('quit', 1), ('scottsdale', 1), ('north', 1), ('crisp', 1), ('philadelphia', 1), ('wish', 1), ('miss', 1), ('expanded', 1), ('café', 1), ('crema', 1), ('trips', 1), ('itll', 1), ('croutons', 1), ('instead', 1), ('pale', 1), ('crumby', 1), ('veganveggie', 1), ('accomodate', 1), ('fo', 1), ('play', 1), ('sugar', 1), ('powdered', 1), ('dusted', 1), ('lightly', 1), ('toast', 1), ('slices', 1), ('giant', 1), ('perpared', 1), ('olives', 1), ('15', 1), ('sucker', 1), ('efficient', 1), ('deeply', 1), ('swung', 1), ('regularly', 1), ('guests', 1), ('witnessed', 1), ('overhaul', 1), ('complete', 1), ('eew', 1), ('theft', 1), ('consider', 1), ('forth', 1), ('hankering', 1), ('palm', 1), ('hearts', 1), ('greens', 1), ('suffers', 1), ('particular', 1), ('disgraceful', 1), ('saving', 1), ('donut', 1), ('strange', 1), ('hello', 1), ('strangers', 1), ('carpaccio', 1), ('yellowtail', 1), ('vacant', 1), ('thirty', 1), ('sample', 1), ('gc', 1), ('opinion', 1), ('martini', 1), ('frenchman', 1), ('experienced', 1), ('meats', 1), ('bunch', 1), ('sun', 1), ('upway', 1), ('cannoli', 1), ('tiramisu', 1), ('togo', 1), ('pack', 1), ('expensive', 1), ('45', 1), ('apology', 1), ('man', 1), ('delivery', 1), ('similarly', 1), ('might', 1), ('unhealthy', 1), ('greasy', 1), ('havent', 1), ('beauty', 1), ('item', 1), ('finger', 1), ('glance', 1), ('typical', 1), ('cheesecurds', 1), ('pastas', 1), ('thoroughly', 1), ('lettuce', 1), ('gyro', 1), ('noca', 1), ('airline', 1), ('fluffy', 1), ('pecan', 1), ('pumpkin', 1), ('multigrain', 1), ('moms', 1), ('funny', 1), ('monster', 1), ('crawfish', 1), ('pancake', 1), ('struck', 1), ('tragedy', 1), ('specials', 1), ('daily', 1), ('conclusion', 1), ('callings', 1), ('name', 1), ('mebunch', 1), ('worker', 1), ('humiliated', 1), ('decide', 1), ('sign', 1), ('gas', 1), ('attached', 1), ('truffle', 1), ('wagyu', 1), ('thinly', 1), ('extraordinary', 1), ('cavier', 1), ('tartare', 1), ('toro', 1), ('boiled', 1), ('limited', 1), ('living', 1), ('nearly', 1), ('hoping', 1), ('exactly', 1), ('las', 1), ('jewel', 1), ('beautifully', 1), ('handled', 1), ('claimed', 1), ('70', 1), ('trimmed', 1), ('cartel', 1), ('shop', 1), ('camelback', 1), ('letdown', 1), ('upgrading', 1), ('ventilation', 1), ('forgetting', 1), ('foodand', 1), ('due', 1), ('carlys', 1), ('az', 1), ('pastry', 1), ('choux', 1), ('profiterole', 1), ('smooth', 1), ('vanilla', 1), ('southwest', 1), ('tater', 1), ('nargile', 1), ('etc', 1), ('groups', 1), ('couples', 1), ('venue', 1), ('relaxed', 1), ('enjoyable', 1), ('mouthful', 1), ('contained', 1), ('certainly', 1), ('difference', 1), ('connoisseur', 1), ('gooodd', 1), ('mesquite', 1), ('ribeye', 1), ('prefer', 1), ('spicier', 1), ('winner', 1), ('almonds', 1), ('pears', 1), ('mention', 1), ('breakfastlunch', 1), ('fs', 1), ('dollars', 1), ('steiners', 1), ('fat', 1), ('gristle', 1), ('34ths', 1), ('15lb', 1), ('awkward', 1), ('absolutley', 1), ('appetizer', 1), ('across', 1), ('rushed', 1), ('wants', 1), ('strike', 1), ('jerk', 1), ('topic', 1), ('expertconnisseur', 1), ('website', 1), ('operation', 1), ('furthermore', 1), ('poop', 1), ('bird', 1), ('pile', 1), ('beensteppedinandtrackedeverywhere', 1), ('smeared', 1), ('ground', 1), ('meeverything', 1), ('companions', 1), ('sore', 1), ('teeth', 1), ('hungry', 1), ('iced', 1), ('petty', 1), ('frustrated', 1), ('gourmet', 1), ('buldogis', 1), ('francisco', 1), ('san', 1), ('shops', 1), ('reminds', 1), ('screwed', 1), ('accountant', 1), ('bouchon', 1), ('kiddos', 1), ('ians', 1), ('dispenser', 1), ('condiment', 1), ('pneumatic', 1), ('memory', 1), ('joy', 1), ('brings', 1), ('transcendant', 1), ('nay', 1), ('summarize', 1), ('aria', 1), ('shopping', 1), ('crystals', 1), ('located', 1), ('past', 1), ('date', 1), ('andddd', 1), ('continue', 1), ('ethic', 1), ('def', 1), ('disgrace', 1), ('90', 1), ('starving', 1), ('breaks', 1), ('halibut', 1), ('outshining', 1), ('dinners', 1), ('hi', 1), ('says', 1), ('inconsiderate', 1), ('outta', 1), ('bean', 1), ('usual', 1), ('customize', 1), ('elegantly', 1), ('tiny', 1), ('space', 1), ('ball', 1), ('dropped', 1), ('rest', 1), ('ache', 1), ('craving', 1), ('despicable', 1), ('rowdy', 1), ('handling', 1), ('buying', 1), ('turn', 1), ('offered', 1), ('sharply', 1), ('shall', 1), ('ramseys', 1), ('gordon', 1), ('sadly', 1), ('enthusiastic', 1), ('genuinely', 1), ('shouldnt', 1), ('app', 1), ('baseball', 1), ('succulent', 1), ('classywarm', 1), ('chipotle', 1), ('vegasthere', 1), ('tucson', 1), ('driving', 1), ('whatsoever', 1), ('spaghetti', 1), ('delicioso', 1), ('17', 1), ('standard', 1), ('yucky', 1), ('plain', 1), ('replenished', 1), ('covers', 1), ('seat', 1), ('unbelievably', 1), ('truly', 1), ('coupons', 1), ('hut', 1), ('honor', 1), ('400', 1), ('vinaigrette', 1), ('unreal', 1), ('peas', 1), ('eyed', 1), ('shawarrrrrrma', 1), ('returned', 1), ('months', 1), ('exceptional', 1), ('downside', 1), ('inhouse', 1), ('ways', 1), ('public', 1), ('lighter', 1), ('bbq', 1), ('loudly', 1), ('publicly', 1), ('making', 1), ('figured', 1), ('bus', 1), ('untoasted', 1), ('muffin', 1), ('english', 1), ('outdoor', 1), ('charming', 1), ('atrocious', 1), ('dime', 1), ('corporation', 1), ('greedy', 1), ('servicecheck', 1), ('boring', 1), ('box', 1), ('count', 1), ('chip', 1), ('noodles', 1), ('writing', 1), ('rating', 1), ('prepared', 1), ('incredibly', 1), ('yelpers', 1), ('fellow', 1), ('disagree', 1), ('saganaki', 1), ('burned', 1), ('stinks', 1), ('mac', 1), ('skimp', 1), ('goat', 1), ('jenni', 1), ('spots', 1), ('dedicated', 1), ('chefs', 1), ('ironman', 1), ('madison', 1), ('platter', 1), ('veggitarian', 1), ('wildly', 1), ('proclaimed', 1), ('self', 1), ('visited', 1), ('thus', 1), ('exquisite', 1), ('charged', 1), ('recall', 1), ('short', 1), ('tough', 1), ('godfathers', 1), ('wouldve', 1), ('7', 1), ('peanut', 1), ('silently', 1), ('parents', 1), ('workingeating', 1), ('privileged', 1), ('gratitude', 1), ('show', 1), ('effort', 1), ('based', 1), ('mood', 1), ('lighting', 1), ('ayce', 1), ('delights', 1), ('reviewer', 1), ('freezing', 1), ('flavourful', 1), ('providing', 1), ('eyes', 1), ('rolled', 1), ('supposed', 1), ('ms', 1), ('decision', 1), ('hardest', 1), ('sauces', 1), ('mayowell', 1), ('eel', 1), ('descriptions', 1), ('theyd', 1), ('tvs', 1), ('covered', 1), ('walls', 1), ('events', 1), ('sporting', 1), ('yeah', 1), ('uploaded', 1), ('picture', 1), ('apart', 1), ('falling', 1), ('patty', 1), ('drunk', 1), ('worstannoying', 1), ('worlds', 1), ('whenever', 1), ('muststop', 1), ('uninspired', 1), ('mains', 1), ('welcome', 1), ('otto', 1), ('bargain', 1), ('unbelievable', 1), ('dates', 1), ('wrapped', 1), ('bodes', 1), ('hopefully', 1), ('pepperand', 1), ('risotto', 1), ('sprouts', 1), ('bussell', 1), ('nude', 1), ('rinse', 1), ('accordingly', 1), ('rich', 1), ('mojitos', 1), ('jamaican', 1), ('tartar', 1), ('kitchen', 1), ('climbing', 1), ('showed', 1), ('beateous', 1), ('corn', 1), ('yukon', 1), ('mushrooms', 1), ('mixed', 1), ('foot', 1), ('stepped', 1), ('1979', 1), ('lived', 1), ('inviting', 1), ('help', 1), ('serivce', 1), ('dreamed', 1), ('exceeding', 1), ('blows', 1), ('fridays', 1), ('magazine', 1), ('readers', 1), ('voted', 1), ('joeys', 1), ('satisfied', 1), ('missing', 1), ('original', 1), ('topvery', 1), ('bits', 1), ('crepe', 1), ('imagined', 1), ('cocktail', 1), ('raspberry', 1), ('lemon', 1), ('quantity', 1), ('hits', 1), ('rudely', 1), ('dressed', 1), ('verge', 1), ('send', 1), ('asking', 1), ('haunt', 1), ('weekly', 1), ('coziness', 1), ('maintaining', 1), ('hip', 1), ('modern', 1), ('desired', 1), ('leaves', 1), ('inspired', 1), ('judging', 1), ('caught', 1), ('perhaps', 1), ('daughter', 1), ('accommodations', 1), ('thrilled', 1), ('terrific', 1), ('missed', 1), ('soi', 1), ('khao', 1), ('lordy', 1), ('waitresses', 1), ('listed', 1), ('handed', 1), ('menus', 1), ('ingredients', 1), ('avocado', 1), ('spinach', 1), ('salty', 1), ('hella', 1), ('metro', 1), ('handsdown', 1), ('cost', 1), ('diverse', 1), ('relocated', 1), ('leftover', 1), ('free', 1), ('gluten', 1), ('voodoo', 1), ('flavored', 1), ('nigiri', 1), ('highlights', 1), ('crostini', 1), ('vain', 1), ('five', 1), ('forty', 1), ('mandalay', 1), ('middle', 1), ('fiancé', 1), ('ignore', 1), ('attention', 1), ('changing', 1), ('cover', 1), ('top', 1), ('moz', 1), ('blanket', 1), ('shoots', 1), ('bamboo', 1), ('suggestions', 1), ('weekend', 1), ('solid', 1), ('expectations', 1), ('meet', 1), ('capers', 1), ('lox', 1), ('nyc', 1), ('crispy', 1), ('meatloaf', 1), ('massive', 1), ('including', 1), ('vodka', 1), ('penne', 1), ('venturing', 1), ('phenomenal', 1), ('reviewing', 1), ('reasons', 1), ('number', 1), ('liking', 1), ('bitches', 1), ('lack', 1), ('host', 1), ('venture', 1), ('hes', 1), ('son', 1), ('loving', 1), ('guys', 1), ('worries', 1), ('cuisine', 1), ('blandest', 1), ('managed', 1), ('actual', 1), ('sound', 1), ('mess', 1), ('arriving', 1), ('ordering', 1), ('40min', 1), ('yama', 1), ('lets', 1), ('sooooo', 1), ('personable', 1), ('afternoon', 1), ('tigerlilly', 1), ('buffets', 1), ('reduction', 1), ('mussels', 1), ('marys', 1), ('bloddy', 1), ('bone', 1), ('loves', 1), ('forward', 1), ('step', 1), ('casino', 1), ('rock', 1), ('hottest', 1), ('flirting', 1), ('outrageously', 1), ('sexy', 1), ('largely', 1), ('summary', 1), ('omelets', 1), ('unsatisfying', 1), ('tenders', 1), ('ratio', 1), ('spring', 1), ('palate', 1), ('tailored', 1), ('disaster', 1), ('raving', 1), ('cakes', 1), ('pan', 1), ('turkey', 1), ('trap', 1), ('smelled', 1), ('parties', 1), ('relationship', 1), ('experiencing', 1), ('cheated', 1), ('appalling', 1), ('sense', 1), ('common', 1), ('excalibur', 1), ('milk', 1), ('chocolate', 1), ('milkshake', 1), ('jeff', 1), ('tastings', 1), ('included', 1), ('apparently', 1), ('updatewent', 1), ('gringos', 1), ('dos', 1), ('discount', 1), ('military', 1), ('wed', 1), ('handmade', 1), ('cocktails', 1), ('receives', 1), ('wave', 1), ('struggle', 1), ('petrified', 1), ('banana', 1), ('power', 1), ('bottom', 1), ('strings', 1), ('flatlined', 1), ('imaginative', 1), ('itfriendly', 1), ('mein', 1), ('chow', 1), ('sunglasses', 1), ('pink', 1), ('refreshing', 1), ('firehouse', 1), ('generic', 1), ('section', 1), ('rib', 1), ('prime', 1), ('register', 1), ('disgust', 1), ('pucks', 1), ('provided', 1), ('positive', 1), ('fear', 1), ('styrofoam', 1), ('melted', 1), ('whether', 1), ('drag', 1), ('hiro', 1), ('stupid', 1), ('ample', 1), ('redeeming', 1), ('pub', 1), ('grab', 1), ('happier', 1), ('accident', 1), ('blow', 1), ('final', 1), ('23', 1), ('combos', 1), ('sever', 1), ('luke', 1), ('street', 1), ('hole', 1), ('cakeohhh', 1), ('velvet', 1), ('rightthe', 1), ('performed', 1), ('interior', 1), ('less', 1), ('blah', 1), ('burrittos', 1), ('alone', 1), ('cash', 1), ('indicate', 1), ('signs', 1), ('shocked', 1), ('disgusted', 1), ('cranberrymmmm', 1), ('ravoli', 1), ('cod', 1), ('cape', 1), ('wayyy', 1), ('prompt', 1), ('warmer', 1), ('ahead', 1), ('rubber', 1), ('honeslty', 1), ('angry', 1), ('steve', 1), ('rick', 1), ('holiday', 1), ('bank', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Cx5fnzgztFf",
        "colab_type": "code",
        "outputId": "869abd52-b7a8-4a44-b135-019df0fc47e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Select 100 most frequent words\n",
        "top100 = [i[0] for i in sorted_count[:100]]\n",
        "print(top100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['food', 'not', 'place', 'good', 'service', 'great', 'back', 'like', 'go', 'time', 'really', 'best', 'dont', 'ever', 'would', 'also', 'one', 'friendly', 'never', 'nice', 'restaurant', 'no', 'delicious', 'amazing', 'im', 'vegas', 'experience', 'ive', 'came', 'wont', 'disappointed', 'even', 'love', 'minutes', 'us', 'eat', 'get', 'staff', 'pretty', 'going', 'well', 'got', 'definitely', 'much', 'bad', 'first', 'chicken', 'made', 'better', 'think', 'say', 'could', 'pizza', 'always', 'stars', 'salad', 'menu', 'steak', 'wait', 'way', 'ordered', 'worst', 'fresh', 'flavor', 'wasnt', 'sushi', 'times', 'server', 'quality', 'taste', 'didnt', 'want', 'awesome', 'night', 'fantastic', 'went', 'enough', 'burger', 'recommend', 'order', 'come', 'know', 'next', 'meal', 'bland', 'cant', 'buffet', 'feel', 'slow', 'still', 'tasty', 'perfect', 'terrible', 'probably', 'waited', 'excellent', 'atmosphere', 'another', 'everything', 'coming']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0eW89uHI0e4j",
        "colab_type": "code",
        "outputId": "c0895cca-9fd3-4702-89e1-9b09f475315c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# Create matrix with reviews as rows and top 100 words as columns, where each cell is 1 if the word appears in the review and 0 otherwise\n",
        "m = []\n",
        "for i in df['nontokens']: m.append([1 if j in i else 0 for j in top100])\n",
        "print(np.matrix(m))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OHtNSbVM0RzX",
        "colab_type": "code",
        "outputId": "e0ec11e1-7d20-4786-9877-dc6b29b43a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1149
        }
      },
      "cell_type": "code",
      "source": [
        "mdf = pd.DataFrame(m, columns = top100, index = df['nontokens'])\n",
        "mdf['Liked'] = df['Liked'].values\n",
        "print(mdf.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    food  not  place  good  \\\n",
            "nontokens                                                                    \n",
            "wow loved place                                        0    0      1     0   \n",
            "crust not good                                         0    1      0     1   \n",
            "not tasty texture nasty                                0    1      0     0   \n",
            "stopped late may bank holiday rick steve recomm...     0    0      0     0   \n",
            "selection menu great prices                            0    0      0     0   \n",
            "\n",
            "                                                    service  great  back  \\\n",
            "nontokens                                                                  \n",
            "wow loved place                                           0      0     0   \n",
            "crust not good                                            0      0     0   \n",
            "not tasty texture nasty                                   0      0     0   \n",
            "stopped late may bank holiday rick steve recomm...        0      0     0   \n",
            "selection menu great prices                               0      1     0   \n",
            "\n",
            "                                                    like  go  time  ...    \\\n",
            "nontokens                                                           ...     \n",
            "wow loved place                                        0   0     0  ...     \n",
            "crust not good                                         0   1     0  ...     \n",
            "not tasty texture nasty                                0   0     0  ...     \n",
            "stopped late may bank holiday rick steve recomm...     0   0     0  ...     \n",
            "selection menu great prices                            0   0     0  ...     \n",
            "\n",
            "                                                    perfect  terrible  \\\n",
            "nontokens                                                               \n",
            "wow loved place                                           0         0   \n",
            "crust not good                                            0         0   \n",
            "not tasty texture nasty                                   0         0   \n",
            "stopped late may bank holiday rick steve recomm...        0         0   \n",
            "selection menu great prices                               0         0   \n",
            "\n",
            "                                                    probably  waited  \\\n",
            "nontokens                                                              \n",
            "wow loved place                                            0       0   \n",
            "crust not good                                             0       0   \n",
            "not tasty texture nasty                                    0       0   \n",
            "stopped late may bank holiday rick steve recomm...         0       0   \n",
            "selection menu great prices                                0       0   \n",
            "\n",
            "                                                    excellent  atmosphere  \\\n",
            "nontokens                                                                   \n",
            "wow loved place                                             0           0   \n",
            "crust not good                                              0           0   \n",
            "not tasty texture nasty                                     0           0   \n",
            "stopped late may bank holiday rick steve recomm...          0           0   \n",
            "selection menu great prices                                 0           0   \n",
            "\n",
            "                                                    another  everything  \\\n",
            "nontokens                                                                 \n",
            "wow loved place                                           0           0   \n",
            "crust not good                                            0           0   \n",
            "not tasty texture nasty                                   0           0   \n",
            "stopped late may bank holiday rick steve recomm...        0           0   \n",
            "selection menu great prices                               0           0   \n",
            "\n",
            "                                                    coming  Liked  \n",
            "nontokens                                                          \n",
            "wow loved place                                          0      1  \n",
            "crust not good                                           0      0  \n",
            "not tasty texture nasty                                  0      0  \n",
            "stopped late may bank holiday rick steve recomm...       0      1  \n",
            "selection menu great prices                              0      1  \n",
            "\n",
            "[5 rows x 101 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lm4XVkoW4PPx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(mdf, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crERsqX945AM",
        "colab_type": "code",
        "outputId": "f7a8198b-d7ff-42f4-ecd1-403a3d099386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "cols = train.columns[:-1]\n",
        "gnb = MultinomialNB()\n",
        "gnb.fit(train[cols], train['Liked'])\n",
        "y_pred = gnb.predict(test[cols])\n",
        "\n",
        "print(\"Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%\"\n",
        "      .format(\n",
        "          test.shape[0],\n",
        "          (test[\"Liked\"] != y_pred).sum(),\n",
        "          100*(1-(test[\"Liked\"] != y_pred).sum()/test.shape[0])\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mislabeled points out of a total 300 points : 73, performance 75.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ra7e7Ul151XB",
        "colab_type": "code",
        "outputId": "82bb5f9e-b789-4de1-bae9-a0bf4b246b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "confusion_matrix(test['Liked'], y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[112,  31],\n",
              "       [ 42, 115]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}